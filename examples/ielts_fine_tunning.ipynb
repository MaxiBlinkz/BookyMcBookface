{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "172440174eb14be1b3333a21ef8692b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1629c9cba045401190598dc2243724f0",
              "IPY_MODEL_7578cec5b89f48038cef8ced523158a0",
              "IPY_MODEL_98e224ecc1a341288c0c8ba7a1a692a4"
            ],
            "layout": "IPY_MODEL_251fc1ddf7ea4a869bbe39baceb5b57a"
          }
        },
        "1629c9cba045401190598dc2243724f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c02e899b67bc45239ec779cb1cdd7027",
            "placeholder": "​",
            "style": "IPY_MODEL_e41470729e534f2399d37a2ddfee5e03",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7578cec5b89f48038cef8ced523158a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033e947a4e3e4e07b86f54fd40963676",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f341eb6d5b94ddba4a9ce8667b8326c",
            "value": 2
          }
        },
        "98e224ecc1a341288c0c8ba7a1a692a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe72e6426a844a36a87dda60bc761aca",
            "placeholder": "​",
            "style": "IPY_MODEL_0d80f492ce4d4198997e89267c724d9c",
            "value": " 2/2 [00:03&lt;00:00,  1.82s/it]"
          }
        },
        "251fc1ddf7ea4a869bbe39baceb5b57a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02e899b67bc45239ec779cb1cdd7027": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e41470729e534f2399d37a2ddfee5e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "033e947a4e3e4e07b86f54fd40963676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f341eb6d5b94ddba4a9ce8667b8326c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe72e6426a844a36a87dda60bc761aca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d80f492ce4d4198997e89267c724d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ba289a862cd4330a4f5e3f8c8681311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c27d4944f494524a46a7f6feb636f36",
              "IPY_MODEL_a5fc5c047bdd465baa1c7df93d5edb46",
              "IPY_MODEL_89b905cc3ff440579761665991715fc4"
            ],
            "layout": "IPY_MODEL_cb7433385706439ab15819dc12be97f7"
          }
        },
        "6c27d4944f494524a46a7f6feb636f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6b2ce696dcb4b71bf12ac6f909167a3",
            "placeholder": "​",
            "style": "IPY_MODEL_05806584dc2445a59cfe07ce47d9228e",
            "value": "Downloading readme: 100%"
          }
        },
        "a5fc5c047bdd465baa1c7df93d5edb46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0abe106dfc11463aaacec340ee291c4f",
            "max": 5010,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfb88ed6e8174675bb50041d22742e3b",
            "value": 5010
          }
        },
        "89b905cc3ff440579761665991715fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24b14faf20e84ab8a4113a2b134752cd",
            "placeholder": "​",
            "style": "IPY_MODEL_db83f9b3e01c4b87a1250dbf32ca305f",
            "value": " 5.01k/5.01k [00:00&lt;00:00, 407kB/s]"
          }
        },
        "cb7433385706439ab15819dc12be97f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b2ce696dcb4b71bf12ac6f909167a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05806584dc2445a59cfe07ce47d9228e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0abe106dfc11463aaacec340ee291c4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfb88ed6e8174675bb50041d22742e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24b14faf20e84ab8a4113a2b134752cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db83f9b3e01c4b87a1250dbf32ca305f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abad1992c21a4a8493c8609835dbc376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65ea4601453943f3acc6721158122c20",
              "IPY_MODEL_c494b3018813477d8a3a4881a4761505",
              "IPY_MODEL_d5c6f34792ba4611aa58436618fcbf72"
            ],
            "layout": "IPY_MODEL_416e016e019d48e2b6aede9e236507d8"
          }
        },
        "65ea4601453943f3acc6721158122c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba04c28a4a849bb97c6edf252143e80",
            "placeholder": "​",
            "style": "IPY_MODEL_be4a71aec5184d00ad31015709109608",
            "value": "Downloading data: 100%"
          }
        },
        "c494b3018813477d8a3a4881a4761505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_662ec0e63fb2414282305c41620aa652",
            "max": 43101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be882239f69a48f5a0a7fbb8e6368d54",
            "value": 43101
          }
        },
        "d5c6f34792ba4611aa58436618fcbf72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf3e1b5dae9245efae387219b4c66625",
            "placeholder": "​",
            "style": "IPY_MODEL_a857225fdbe94e5f97631dadc381d9fa",
            "value": " 43.1k/43.1k [00:00&lt;00:00, 126kB/s]"
          }
        },
        "416e016e019d48e2b6aede9e236507d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dba04c28a4a849bb97c6edf252143e80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be4a71aec5184d00ad31015709109608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "662ec0e63fb2414282305c41620aa652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be882239f69a48f5a0a7fbb8e6368d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf3e1b5dae9245efae387219b4c66625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a857225fdbe94e5f97631dadc381d9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "455c0cdca6f244e8865093ebe04e83ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0ad1e180f964f4abe5589296b008700",
              "IPY_MODEL_a832769eeed842468471536a67eeb0d6",
              "IPY_MODEL_346a547009bb4d0d888964c790db24ad"
            ],
            "layout": "IPY_MODEL_cf88fb83f22946ed9a2174e0ce1c2bdc"
          }
        },
        "c0ad1e180f964f4abe5589296b008700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9accff4ab2f0481b87573188add0e44d",
            "placeholder": "​",
            "style": "IPY_MODEL_52b0f027bb414f1c8b3fb8d644ce54e6",
            "value": "Generating train split: 100%"
          }
        },
        "a832769eeed842468471536a67eeb0d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87c5ec19a652467998b63203855c22d9",
            "max": 492,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dddd99644ee94fddb4f8b3b436a5f3c4",
            "value": 492
          }
        },
        "346a547009bb4d0d888964c790db24ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_028492fb8b4d483cb13d45491353fd66",
            "placeholder": "​",
            "style": "IPY_MODEL_0fed45330b9b471185fe6d427ecf51f6",
            "value": " 492/492 [00:00&lt;00:00, 21736.04 examples/s]"
          }
        },
        "cf88fb83f22946ed9a2174e0ce1c2bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9accff4ab2f0481b87573188add0e44d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52b0f027bb414f1c8b3fb8d644ce54e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87c5ec19a652467998b63203855c22d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dddd99644ee94fddb4f8b3b436a5f3c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "028492fb8b4d483cb13d45491353fd66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fed45330b9b471185fe6d427ecf51f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaxiBlinkz/BookyMcBookface/blob/master/examples/ielts_fine_tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/simplifine-llm/Simplifine/blob/main/examples/cloud_quickstart.ipynb)\n",
        "### 📦 Installing Required Libraries\n",
        "\n",
        "Before we begin fine-tuning our fake news detector, we need to install the necessary libraries. In this step, we’re installing the `Simplifine` library, which provides tools to streamline the fine-tuning process for large language models. We’re also installing the `datasets` library, which allows us to easily access and manage datasets from Hugging Face.\n",
        "\n",
        "- The `Simplifine` library helps in making the fine-tuning process more efficient, whether you're working locally or in the cloud.\n",
        "- The `datasets` library is essential for loading and processing the dataset we'll be using for this project.\n",
        "\n",
        "Running this cell will install both libraries quietly in the background.\n"
      ],
      "metadata": {
        "id": "0SClYIzAQrpD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxDXEqYrw-gh",
        "outputId": "634549fb-f091-4f4a-c838-20312c03da94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.3/318.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for simplifine-alpha (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/simplifine-llm/Simplifine.git -q\n",
        "!pip install datasets -q\n",
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🛠️ Setting Up for Local Training\n",
        "\n",
        "In this section, we’re preparing to fine-tune our fake news detector model using Google Colab’s resources. The steps below outline how to configure and initiate the training process.\n",
        "\n",
        "1. **Importing Libraries:**\n",
        "   - We import `train_engine` from the `Simplifine` library, which provides the necessary functions to handle the fine-tuning process.\n",
        "   - We also import `SFTConfig` from the `trl` library, which allows us to configure the supervised fine-tuning parameters.\n",
        "\n",
        "2. **Dataset Selection:**\n",
        "   - We define the dataset name as `'community-datasets/fake_news_english'`. This dataset contains examples of fake news articles that we will use to fine-tune our model.\n",
        "\n",
        "3. **Prompt Configuration:**\n",
        "   - We create a `sftPromptConfig` object to specify how the training data is formatted.\n",
        "   - The `template` parameter defines the input format, and the `response_template` specifies how the model should generate outputs.\n",
        "   - The `use_chat_template` flag is set to `True` to format the inputs in a conversational style, which can be effective for chat-based models.\n",
        "\n",
        "4. **Training Configuration:**\n",
        "   - We define the training settings using `SFTConfig`. This includes parameters like batch size, learning rate, and the number of epochs.\n",
        "   - We also enable `fp16` (16-bit floating-point) training for faster computation and set `gradient_checkpointing` to save memory during training.\n",
        "\n",
        "5. **Model Selection:**\n",
        "   - The model we’re fine-tuning is `'TinyLlama/TinyLlama-1.1B-Chat-v1.0'`. This is a smaller, efficient model suitable for demonstration purposes on Colab.\n",
        "\n",
        "6. **Training the Model:**\n",
        "   - Finally, we call `sft_train` to start the fine-tuning process. This step will take a while to complete, as we’re training the model from scratch without any optimizations like quantization or LoRA.\n",
        "\n",
        "Running this cell will fine-tune the model locally on Colab, using the configurations we’ve set up. This is ideal for quick experiments or when cloud resources are not available."
      ],
      "metadata": {
        "id": "C0dDwmg4Rb3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simplifine_alpha import train_engine\n",
        "from trl import SFTConfig\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the training dataset\n",
        "train_dataset = load_dataset('csv', data_files='filtered_df_train.csv')\n",
        "\n",
        "# Load the testing dataset\n",
        "test_dataset = load_dataset('csv', data_files='filtered_df_test.csv')\n",
        "\n",
        "# Define prompt config\n",
        "sft_prompt_config = train_engine.sftPromptConfig(\n",
        "  keys=['prompt', 'essay'],\n",
        "  template=\"### Prompt: {prompt}\\n### Essay: {essay}\",\n",
        "  response_template=\". \\n### Feedback:\",\n",
        "  use_chat_template=True\n",
        "  )\n",
        "\n",
        "# Define training config, increase max length to accommodate longer responses\n",
        "sft_config = SFTConfig(\n",
        "    output_dir='/content/ielts_review_llama3',\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=2,\n",
        "    report_to='none',\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    max_length=4096,  # Increased sequence length for training\n",
        ")\n",
        "\n",
        "# Select the Llama 3 1B model\n",
        "model_name = 'decapoda-research/llama-3-1b-hf'\n",
        "\n",
        "# Fine-tune the model\n",
        "train_engine.sft_train(model_name=model_name,\n",
        "                       dataset=train_dataset, # using training dataset\n",
        "                       sft_config=sft_config,\n",
        "                       sft_prompt_config=sft_prompt_config,\n",
        "                       use_zero=False,\n",
        "                       use_ddp=False\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "uKH1cxpkxFAr",
        "outputId": "0d4f383d-cba3-49f8-d2e2-48ea73953c0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Unable to find '/content/filtered_df_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c92b0f71a433>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'filtered_df_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the testing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2130\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1562\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m         ).get_module()\n\u001b[0m\u001b[1;32m   1565\u001b[0m     \u001b[0;31m# Try locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         )\n\u001b[0;32m--> 944\u001b[0;31m         data_files = DataFilesDict.from_patterns(\n\u001b[0m\u001b[1;32m    945\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFilesList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 else DataFilesList.from_patterns(\n\u001b[0m\u001b[1;32m    722\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 data_files.extend(\n\u001b[0;32m--> 624\u001b[0;31m                     resolve_pattern(\n\u001b[0m\u001b[1;32m    625\u001b[0m                         \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                         \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/content/filtered_df_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ☁️ Training the Model on Cloud Servers\n",
        "\n",
        "In this section, we’re moving from local training to cloud-based training using Simplifine’s cloud infrastructure. This allows you to leverage powerful GPUs like the A100 for more intensive tasks, making it easier to handle larger models and datasets.\n",
        "\n",
        "1. **Importing the `train_utils` Module:**\n",
        "   - We start by importing the `train_utils` module from the `Simplifine` library. This module provides utilities to interact with Simplifine's cloud servers.\n",
        "\n",
        "2. **Model and API Configuration:**\n",
        "   - We select a different model for this cloud training: `'microsoft/Phi-3-mini-4k-instruct'`. This model is more powerful and well-suited for deployment on cloud GPUs.\n",
        "   - The `simplifine_api_key` is your unique key to access Simplifine’s cloud services. Ensure you have it ready.\n",
        "   - The `gpu_type` is set to `'a100'`, which specifies the type of GPU to be used in the cloud. The A100 is a high-performance GPU ideal for deep learning tasks.\n",
        "\n",
        "   ### 🔑 Need an API Key?\n",
        "   If you don't have an API key yet, you can [**request one here for free**](https://www.simplifine.com/api-key-interest). The turnaround time is just 24 hours, so you'll be up and running in no time!\n",
        "\n",
        "3. **Client Initialization:**\n",
        "   - We create a `Client` object using the API key and GPU type. This client will handle the communication with Simplifine’s cloud infrastructure, managing the training job on your behalf.\n",
        "\n",
        "4. **Defining the Training Job:**\n",
        "   - The `job_name` is set to `'fake_news_english_phi3'`, which uniquely identifies this training task.\n",
        "   - We then call the `sft_train_cloud` method on our `client` object. This method sends the training job to the cloud, using the model and configurations we’ve defined earlier.\n",
        "\n",
        "5. **Cloud Training Setup:**\n",
        "   - We enable `use_zero=True` to utilize DeepSpeed's ZeRO optimization, allowing the model to scale effectively across multiple GPUs.\n",
        "   - We disable Distributed Data Parallel (DDP) for this job, which is appropriate when ZeRO is handling the distribution of data.\n",
        "\n",
        "Running this cell will initiate the training process on Simplifine’s cloud servers, allowing you to offload the heavy lifting to a powerful cloud infrastructure. This is ideal when working with larger models or when your local resources are insufficient.\n"
      ],
      "metadata": {
        "id": "oehMA7hwRky5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simplifine_alpha import train_utils\n",
        "\n",
        "# change name to phi 3\n",
        "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "simplifine_api_key = 'PUT YOUR OWN API KEY PROVIDED BY SIMPLIFINE'\n",
        "gpu_type = 'a100'\n",
        "client = train_utils.Client(simplifine_api_key, gpu_type)\n",
        "\n",
        "job_name = 'fake_news_english_phi3'\n",
        "\n",
        "\n",
        "client.sft_train_cloud(job_name=job_name, model_name=model_name, dataset_name=dataset_name,\n",
        "                       sft_config = sft_config, sft_prompt_config=sft_prompt_config,\n",
        "                       use_zero=True, use_ddp=False\n",
        "                      )"
      ],
      "metadata": {
        "id": "O1zdn8r85n-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2510f4d-5246-4631-df37-8a741cf92240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-08-07 18:34:35,105] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
            "[2024-08-07 18:34:35,110] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📝 Checking the Status of Your Training Jobs\n",
        "\n",
        "After submitting your training job to Simplifine’s cloud servers, it’s important to monitor its status to ensure everything is running smoothly. In this section, we’ll check the status of your most recent job.\n",
        "\n",
        "1. **Retrieving Job Status:**\n",
        "   - We call the `get_all_jobs` method on our `client` object. This method returns a list of all jobs associated with your API key, including their current statuses.\n",
        "\n",
        "2. **Displaying the Latest Job:**\n",
        "   - We loop through the latest job in the list and print its status. This gives you a quick overview of how your most recent training job is progressing.\n",
        "\n",
        "3. **Understanding Job Statuses:**\n",
        "   - Your job can have one of the following statuses:\n",
        "     - `pending`: The job has been submitted and is waiting to start.\n",
        "     - `in progress`: The job is currently running.\n",
        "     - `stopped`: The job was stopped before completion, either manually or due to an error.\n",
        "     - `completed`: The job has successfully finished.\n",
        "\n",
        "Running this cell will display the status of your most recent job, helping you keep track of your training tasks on Simplifine’s cloud servers.\n"
      ],
      "metadata": {
        "id": "W88J_Ef7yaYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "status = client.get_all_jobs()\n",
        "for num,i in enumerate(status[-1:]):\n",
        "  print(f'Job {num}: {i}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l70vZyPV6_AC",
        "outputId": "b32db3fe-e353-4105-e8b7-63a772d7ccde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job 0: {'job_id': '183c65ad-2b4e-4d11-b2a5-d66232d5b15b', 'job_name': 'fake_news_english_phi3', 'status': 'completed'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📊 Retrieving and Viewing Training Logs\n",
        "\n",
        "After checking the status of your training job, you might want to dive deeper into the details by viewing the training logs. These logs provide insights into the training process, including any issues or updates on the progress.\n",
        "\n",
        "1. **Getting the `job_id`:**\n",
        "   - We start by extracting the `job_id` of the last job from the status list. The `job_id` is a unique identifier for each training job, which we’ll use to retrieve its logs.\n",
        "\n",
        "2. **Retrieving Logs:**\n",
        "   - We call the `get_train_logs` method on our `client` object, passing in the `job_id`. This method fetches the detailed logs for the specified job, giving you access to the complete training history.\n",
        "\n",
        "3. **Viewing the Logs:**\n",
        "   - Finally, we print the `response` from the logs, which contains detailed information about the training process. This includes updates, errors, and any other relevant messages from the training run.\n",
        "\n",
        "Running this cell will display the logs for your most recent job, allowing you to monitor and troubleshoot the training process effectively.\n"
      ],
      "metadata": {
        "id": "BDe93gbayl_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the job_id of the last job\n",
        "job_id = status[-1]['job_id']\n",
        "\n",
        "logs = client.get_train_logs(job_id)\n",
        "print(logs['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt35FPNn8ADK",
        "outputId": "1de668ed-718e-452d-eb85-0632d7652008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W0806 18:14:41.510000 129132731527296 torch/distributed/run.py:779] \n",
            "W0806 18:14:41.510000 129132731527296 torch/distributed/run.py:779] *****************************************\n",
            "W0806 18:14:41.510000 129132731527296 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0806 18:14:41.510000 129132731527296 torch/distributed/run.py:779] *****************************************\n",
            "[2024-08-06 18:14:46,878] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-08-06 18:14:46,910] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "[2024-08-06 18:14:46,961] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "[2024-08-06 18:14:47,065] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_fwd\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_bwd\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "[2024-08-06 18:14:47,135] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_fwd\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_bwd\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "[2024-08-06 18:14:47,158] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-08-06 18:14:47,172] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-08-06 18:14:47,194] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_fwd\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_bwd\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_fwd\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_bwd\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_fwd\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_bwd\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_fwd\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_bwd\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_fwd\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_bwd\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_fwd\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @autocast_custom_bwd\n",
            "[2024-08-06 18:14:48,688] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-08-06 18:14:48,695] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-08-06 18:14:48,785] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-08-06 18:14:48,850] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-08-06 18:14:48,890] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "Destroying existing process group\n",
            "Destroying existing process group\n",
            "[2024-08-06 18:14:48,922] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-08-06 18:14:48,946] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-08-06 18:14:48,947] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "Destroying existing process group\n",
            "Destroying existing process group\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Destroying existing process group\n",
            "Destroying existing process group\n",
            "Destroying existing process group\n",
            "Destroying existing process group\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "\n",
            "Map:   0%|          | 0/393 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/393 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/393 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/393 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/393 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/393 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/393 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/393 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 662.32 examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 670.82 examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 655.96 examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 654.94 examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 670.00 examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 674.59 examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 697.91 examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 635.21 examples/s]\n",
            "\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 637.09 examples/s]\n",
            "\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 624.12 examples/s]\n",
            "\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 622.66 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/99 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/99 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 626.62 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/99 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 645.62 examples/s]\n",
            "\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 616.65 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/99 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 697.04 examples/s]\n",
            "Map:   0%|          | 0/99 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/99 [00:00<?, ? examples/s]\n",
            "Map:   0%|          | 0/99 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 393/393 [00:00<00:00, 663.68 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/99 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 835.60 examples/s]\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 794.46 examples/s]\n",
            "\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 837.59 examples/s]Using CUDA\n",
            "\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 845.48 examples/s]\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 796.62 examples/s]\n",
            "\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 804.56 examples/s]\n",
            "Using CUDA\n",
            "Using CUDA\n",
            "\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 853.92 examples/s]\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 811.99 examples/s]\n",
            "Using CUDA\n",
            "\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 852.33 examples/s]\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 810.34 examples/s]\n",
            "Using CUDA\n",
            "\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 862.42 examples/s]\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 820.15 examples/s]\n",
            "\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 847.22 examples/s]Using CUDA\n",
            "\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 802.41 examples/s]\n",
            "Using CUDA\n",
            "\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 871.67 examples/s]\n",
            "Map: 100%|██████████| 99/99 [00:00<00:00, 828.15 examples/s]\n",
            "Using CUDA\n",
            "\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.07s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.10s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.11s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.09s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.07s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.11s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.09s/it]\n",
            "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.15s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.44s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.68s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.45s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.44s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.69s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.45s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.46s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.74s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.74s/it]\n",
            "\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.78s/it]\n",
            "Initializing process group for DDP\n",
            "using ZeRO optimization\n",
            "train data set is: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 393\n",
            "}), eval dataset is Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 99\n",
            "})\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Initializing process group for DDP\n",
            "using ZeRO optimization\n",
            "train data set is: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 393\n",
            "}), eval dataset is Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 99\n",
            "})\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:494: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a valid formatting function. Therefore `formatting_func` will be ignored.\n",
            "  warnings.warn(\n",
            "Initializing process group for DDP\n",
            "using ZeRO optimization\n",
            "train data set is: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 393\n",
            "}), eval dataset is Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 99\n",
            "})\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Initializing process group for DDP\n",
            "using ZeRO optimization\n",
            "train data set is: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 393\n",
            "}), eval dataset is Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 99\n",
            "})\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Initializing process group for DDP\n",
            "using ZeRO optimization\n",
            "train data set is: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 393\n",
            "}), eval dataset is Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 99\n",
            "})\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Initializing process group for DDP\n",
            "using ZeRO optimization\n",
            "train data set is: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 393\n",
            "}), eval dataset is Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 99\n",
            "})\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Initializing process group for DDP\n",
            "using ZeRO optimization\n",
            "train data set is: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 393\n",
            "}), eval dataset is Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 99\n",
            "})\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "Initializing process group for DDP\n",
            "using ZeRO optimization\n",
            "train data set is: Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 393\n",
            "}), eval dataset is Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 99\n",
            "})\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:494: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a valid formatting function. Therefore `formatting_func` will be ignored.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:494: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a valid formatting function. Therefore `formatting_func` will be ignored.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:494: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a valid formatting function. Therefore `formatting_func` will be ignored.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:494: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a valid formatting function. Therefore `formatting_func` will be ignored.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:494: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a valid formatting function. Therefore `formatting_func` will be ignored.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:494: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a valid formatting function. Therefore `formatting_func` will be ignored.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:494: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a valid formatting function. Therefore `formatting_func` will be ignored.\n",
            "  warnings.warn(\n",
            "Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "Using /home/ubuntu/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
            "Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "Using /home/ubuntu/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
            "Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "Using /home/ubuntu/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
            "Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "Using /home/ubuntu/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
            "Using /home/ubuntu/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
            "Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "Using /home/ubuntu/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
            "Using /home/ubuntu/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
            "Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "Using /home/ubuntu/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
            "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py312_cu121/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 2.368800640106201 seconds\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 2.408421277999878 seconds\n",
            "Loading extension module cpu_adam...\n",
            "Loading extension module cpu_adam...\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 2.411547899246216 seconds\n",
            "Time to load cpu_adam op: 2.41204571723938 seconds\n",
            "Time to load cpu_adam op: 2.405897855758667 seconds\n",
            "Loading extension module cpu_adam...\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 2.413489818572998 seconds\n",
            "Time to load cpu_adam op: 2.417717218399048 seconds\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 2.420786142349243 seconds\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "\n",
            "  0%|          | 0/24 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/home/ubuntu/mlenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "\n",
            "  4%|▍         | 1/24 [00:29<11:14, 29.32s/it]\n",
            "  8%|▊         | 2/24 [00:53<09:38, 26.31s/it]\n",
            " 12%|█▎        | 3/24 [01:17<08:54, 25.45s/it]\n",
            " 17%|█▋        | 4/24 [01:42<08:25, 25.27s/it]\n",
            " 21%|██        | 5/24 [02:10<08:13, 26.00s/it]\n",
            " 25%|██▌       | 6/24 [02:34<07:35, 25.30s/it]\n",
            " 29%|██▉       | 7/24 [02:58<07:02, 24.88s/it]\n",
            " 33%|███▎      | 8/24 [03:23<06:40, 25.02s/it]\n",
            " 38%|███▊      | 9/24 [03:47<06:10, 24.70s/it]\n",
            " 42%|████▏     | 10/24 [04:11<05:42, 24.43s/it]\n",
            " 46%|████▌     | 11/24 [04:34<05:13, 24.14s/it]\n",
            " 50%|█████     | 12/24 [04:59<04:50, 24.22s/it]\n",
            " 54%|█████▍    | 13/24 [05:23<04:26, 24.23s/it]\n",
            " 58%|█████▊    | 14/24 [05:47<04:00, 24.04s/it]\n",
            " 62%|██████▎   | 15/24 [06:09<03:33, 23.67s/it]\n",
            " 67%|██████▋   | 16/24 [06:32<03:07, 23.44s/it]\n",
            " 71%|███████   | 17/24 [06:56<02:44, 23.56s/it]\n",
            " 75%|███████▌  | 18/24 [07:19<02:20, 23.47s/it]\n",
            " 79%|███████▉  | 19/24 [07:43<01:58, 23.62s/it]\n",
            " 83%|████████▎ | 20/24 [08:07<01:34, 23.60s/it]\n",
            "                                               \n",
            "{'loss': 1.3721, 'grad_norm': 0.6817618012428284, 'learning_rate': 4.0909090909090915e-06, 'epoch': 1.6}\n",
            "\n",
            " 83%|████████▎ | 20/24 [08:07<01:34, 23.60s/it]\n",
            " 88%|████████▊ | 21/24 [08:30<01:10, 23.57s/it]\n",
            " 92%|█████████▏| 22/24 [08:53<00:46, 23.38s/it]\n",
            " 96%|█████████▌| 23/24 [09:17<00:23, 23.53s/it]\n",
            "100%|██████████| 24/24 [09:41<00:00, 23.53s/it]\n",
            "                                               \n",
            "{'train_runtime': 613.9336, 'train_samples_per_second': 1.28, 'train_steps_per_second': 0.039, 'train_loss': 1.1531557595978181, 'epoch': 1.92}\n",
            "\n",
            "100%|██████████| 24/24 [10:13<00:00, 23.53s/it]\n",
            "100%|██████████| 24/24 [10:13<00:00, 25.58s/it]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📂 Downloading and Saving the Trained Model\n",
        "\n",
        "Once your training job is completed, the next step is to download the trained model so you can use it locally or for further fine-tuning.\n",
        "\n",
        "1. **Creating a Directory for the Model:**\n",
        "   - We begin by creating a new folder called `sf_trained_model_zero_phi`. This folder will serve as the destination for the downloaded model files.\n",
        "\n",
        "2. **Downloading the Model:**\n",
        "   - We use the `download_model` method on our `client` object to download the trained model from the cloud. The `job_id` is passed to specify which model to download, and we extract the files to the newly created directory.\n",
        "   \n",
        "   - **Tip:** This process might take some time depending on the size of the model, so feel free to take a break or grab a coffee while you wait! ☕\n",
        "\n",
        "Running this cell will download your trained model and save it in the specified directory, making it ready for use in your next project or analysis.\n"
      ],
      "metadata": {
        "id": "koKpp2XNU-y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# creating a folder to store the model\n",
        "os.mkdir('sf_trained_model_zero_phi')\n",
        "\n",
        "# download and save the model to it.\n",
        "# This might take some time, have a sip of that coffee! :)\n",
        "client.download_model(job_id=job_id, extract_to='/content/sf_trained_model_zero_phi')"
      ],
      "metadata": {
        "id": "DNv9XkFv80d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88812a9-8f64-464e-d4c5-d2ace8814f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 6.99G/6.99G [00:42<00:00, 166MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Directory downloaded successfully and saved to /content/sf_trained_model_zero_phi/183c65ad-2b4e-4d11-b2a5-d66232d5b15b.zip\n",
            "Model unzipped successfully to /content/sf_trained_model_zero_phi\n",
            "Deleted the zip file at /content/sf_trained_model_zero_phi/183c65ad-2b4e-4d11-b2a5-d66232d5b15b.zip\n",
            "Model downloaded, unzipped, and zip file deleted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔄 Loading the Trained Model and Tokenizer\n",
        "\n",
        "Now that we've successfully downloaded the trained model, the next step is to load it into our environment so we can use it for inference or further fine-tuning.\n",
        "\n",
        "1. **Importing Required Libraries:**\n",
        "   - We import `AutoModelForCausalLM` and `AutoTokenizer` from the `transformers` library. These classes are used to load the model and tokenizer from the saved files.\n",
        "\n",
        "2. **Setting the Path:**\n",
        "   - We set the `path` variable to point to the directory where we saved the trained model (`'/content/sf_trained_model_zero_phi'`).\n",
        "\n",
        "3. **Loading the Model:**\n",
        "   - We use `AutoModelForCausalLM.from_pretrained(path)` to load the trained model from the specified path. This initializes the model so it’s ready for use.\n",
        "\n",
        "4. **Loading the Tokenizer:**\n",
        "   - Similarly, we load the tokenizer using `AutoTokenizer.from_pretrained(path)`. The tokenizer is essential for processing text input into a format that the model can understand.\n",
        "\n",
        "Running this cell will load both the trained model and tokenizer into your environment, allowing you to start generating text or continue fine-tuning with your freshly trained model."
      ],
      "metadata": {
        "id": "mQ1fk9tJVJKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "path = '/content/sf_trained_model_zero_phi'\n",
        "sf_model = AutoModelForCausalLM.from_pretrained(path)\n",
        "sf_tokenizer = AutoTokenizer.from_pretrained(path)"
      ],
      "metadata": {
        "id": "WOte32R79n9j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "172440174eb14be1b3333a21ef8692b2",
            "1629c9cba045401190598dc2243724f0",
            "7578cec5b89f48038cef8ced523158a0",
            "98e224ecc1a341288c0c8ba7a1a692a4",
            "251fc1ddf7ea4a869bbe39baceb5b57a",
            "c02e899b67bc45239ec779cb1cdd7027",
            "e41470729e534f2399d37a2ddfee5e03",
            "033e947a4e3e4e07b86f54fd40963676",
            "2f341eb6d5b94ddba4a9ce8667b8326c",
            "fe72e6426a844a36a87dda60bc761aca",
            "0d80f492ce4d4198997e89267c724d9c"
          ]
        },
        "outputId": "9b14f28f-3376-45bc-82cb-c6b09a31aa6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "172440174eb14be1b3333a21ef8692b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📚 Loading the Dataset\n",
        "\n",
        "Before we can use our trained model for inference or further fine-tuning, we need to load the dataset that we’ve been working with.\n",
        "\n",
        "1. **Importing the Datasets Library:**\n",
        "   - We start by importing the `datasets` library, which provides easy access to a wide range of datasets, including the one we've been using for training.\n",
        "\n",
        "2. **Loading the Dataset:**\n",
        "   - We load the dataset using the `load_dataset` function from the `datasets` library. The `dataset_name` variable contains the name of the dataset we specified earlier in our code.\n",
        "\n",
        "Running this cell will load the dataset into your environment, making it ready for evaluation, inference,"
      ],
      "metadata": {
        "id": "UZ-1si0bVOMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "dataset = datasets.load_dataset(dataset_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "9ba289a862cd4330a4f5e3f8c8681311",
            "6c27d4944f494524a46a7f6feb636f36",
            "a5fc5c047bdd465baa1c7df93d5edb46",
            "89b905cc3ff440579761665991715fc4",
            "cb7433385706439ab15819dc12be97f7",
            "e6b2ce696dcb4b71bf12ac6f909167a3",
            "05806584dc2445a59cfe07ce47d9228e",
            "0abe106dfc11463aaacec340ee291c4f",
            "bfb88ed6e8174675bb50041d22742e3b",
            "24b14faf20e84ab8a4113a2b134752cd",
            "db83f9b3e01c4b87a1250dbf32ca305f",
            "abad1992c21a4a8493c8609835dbc376",
            "65ea4601453943f3acc6721158122c20",
            "c494b3018813477d8a3a4881a4761505",
            "d5c6f34792ba4611aa58436618fcbf72",
            "416e016e019d48e2b6aede9e236507d8",
            "dba04c28a4a849bb97c6edf252143e80",
            "be4a71aec5184d00ad31015709109608",
            "662ec0e63fb2414282305c41620aa652",
            "be882239f69a48f5a0a7fbb8e6368d54",
            "cf3e1b5dae9245efae387219b4c66625",
            "a857225fdbe94e5f97631dadc381d9fa",
            "455c0cdca6f244e8865093ebe04e83ee",
            "c0ad1e180f964f4abe5589296b008700",
            "a832769eeed842468471536a67eeb0d6",
            "346a547009bb4d0d888964c790db24ad",
            "cf88fb83f22946ed9a2174e0ce1c2bdc",
            "9accff4ab2f0481b87573188add0e44d",
            "52b0f027bb414f1c8b3fb8d644ce54e6",
            "87c5ec19a652467998b63203855c22d9",
            "dddd99644ee94fddb4f8b3b436a5f3c4",
            "028492fb8b4d483cb13d45491353fd66",
            "0fed45330b9b471185fe6d427ecf51f6"
          ]
        },
        "id": "Orm2RTPh1s-s",
        "outputId": "34794037-e2bb-4e64-cf52-445e61a7aaf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/5.01k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ba289a862cd4330a4f5e3f8c8681311"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/43.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abad1992c21a4a8493c8609835dbc376"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/492 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "455c0cdca6f244e8865093ebe04e83ee"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 Generating Text with the Trained Model\n",
        "\n",
        "Now that we've loaded both the model and the dataset, it’s time to generate some text using our trained model. In this section, we’ll configure the generation settings and produce some sample outputs.\n",
        "\n",
        "1. **Importing Inference Tools:**\n",
        "   - We import `inference_tools` from the `simplifine_alpha` library. This module provides the necessary tools to generate text using the model we’ve fine-tuned.\n",
        "\n",
        "2. **Configuring Text Generation:**\n",
        "   - We create a `GenerationConfig` object to define how the model should generate text. This configuration includes:\n",
        "     - `prompt_template` and `response_template`: Templates for how the inputs and outputs are formatted.\n",
        "     - `keys`: Specifies the data keys used in the templates.\n",
        "     - `train_type`: Indicates that we're using supervised fine-tuning (`sft`).\n",
        "     - `max_length`: The maximum length of the generated sequences.\n",
        "     - `num_return_sequences`: How many sequences to generate.\n",
        "     - `do_sample`, `top_k`, `top_p`, `temperature`: Parameters that control the randomness and diversity of the generated text.\n",
        "\n",
        "3. **Generating Text:**\n",
        "   - We call `generate_from_pretrained` using our fine-tuned model, tokenizer, and the generation configuration. We also pass in a small sample of the dataset to generate text based on the training data.\n",
        "   \n",
        "   - **Note:** We’re using only the first three examples from the training dataset (`dataset['train'][:3]`) for quick testing.\n",
        "\n",
        "4. **Displaying the Generated Text:**\n",
        "   - Finally, we print the generated text, which provides a glimpse into how well the model has learned to detect fake news.\n",
        "\n",
        "Running this cell will generate text using your trained model, showcasing its ability to produce outputs based on the fine-tuned dataset. This is where you can see the real impact of your training efforts!"
      ],
      "metadata": {
        "id": "tHGpRwU6VVav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simplifine_alpha import inference_tools\n",
        "\n",
        "# Configuration for generating text with longer sequences\n",
        "config = inference_tools.GenerationConfig(\n",
        "    prompt_template=sft_prompt_config.template,\n",
        "    response_template=sft_prompt_config.response_template,\n",
        "    keys=sft_prompt_config.keys,\n",
        "    train_type='sft',\n",
        "    max_length=4096,  # Increased sequence length for generation\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    temperature=0.99\n",
        ")\n",
        "\n",
        "generated_text = inference_tools.generate_from_pretrained(sf_model, sf_tokenizer, config, data=test_dataset['train'][:3]) # using test dataset\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KWnTV9w1OMQ",
        "outputId": "e78d14ca-9b91-4412-8d16-ece24b3ffe7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['###URL: http://www.redflagnews.com/headlines-2016/cdc-proposes-rule-to-apprehend-and-detain-anyone-anywhere-at-any-time-for-any-duration-without-due-process-or-right-of-appeal-and-administer-forced-vaccinations-or-medical-treatment-without-consent-or-parens. \\n###'], ['###URL: http://www.redflagnews.com/headlines-2016/-outrage-what-obama-just-did-to-the-white-house-logo-will-make-you-sick-128097.html \\n###CLS: 0'], ['###URL: http://www.redflagnews.com/headlines-2016/white-house-cancels-all-obama-appearances-at-hillary-campaign-events-as-he-navigates-mandatory-divorce-june-28-2016-1651142.html \\n###CLS: 1']]\n"
          ]
        }
      ]
    }
  ]
}